
services:
  
  vllm:
    image: rocm/vllm:latest
    devices:
      - /dev/kfd
      - /dev/dri
    security_opt:
      - seccomp:unconfined
    
    group_add:
      - "video"
      - "render"
    ipc: 
      "host"
    ports: 
      - "8001:8001"
    cap_add:
      - SYS_PTRACE
    environment: 
      HIP_VISIBLE_DEVICES: 4
      #ROCR_VISIBLE_DEVICES: 5,6

      # one of these makes it work and idk which ones
      # VLLM_SKIP_P2P_CHECK: 1
      # NCCL_CUMEM_ENABLE: 0
      # NCCL_IB_DISABLE: 1
      HSA_ENABLE_IPC_MODE_LEGACY: 0
      # why not have all of them 
      VLLM_USE_TRITON_FLASH_ATTN: 0
      HF_HUB_CACHE: /hf_home

    volumes:
      - /shared/huggingface:/hf_home
    command: ["/bin/sh", 
              "-c", 
              "vllm serve microsoft/Phi-4-multimodal-instruct --port 8001 --trust_remote_code --enable-lora --max-lora-rank 320 --lora-extra-vocab-size 0 --limit-mm-per-prompt audio=3,image=3 --max-loras 2 --lora-modules speech=/speech-lora vision=/vision-lora
              "]
      # --tensor-parallel-size 2
      # --max-model-len 74782
      # Salesforce/Llama-xLAM-2-70b-fc-r
      #         --enable-auto-tool-choice
      #         --tool-parser-plugin ./xlam_tool_call_parser.py
      #         --tool-call-parser xlam
  
  # vision:
  #   image: rocm/vllm:latest
  #   devices:
  #     - /dev/kfd
  #     - /dev/dri
  #   security_opt:
  #     - seccomp:unconfined
    
  #   group_add:
  #     - "video"
  #     - "render"
  #   ipc: 
  #     "host"
  #   ports: 
  #     - "8001:8001"
  #   cap_add:
  #     - SYS_PTRACE
  #   environment: 
  #     HIP_VISIBLE_DEVICES: 5
  #     #ROCR_VISIBLE_DEVICES: 5,6

  #     # one of these makes it work and idk which ones
  #     # VLLM_SKIP_P2P_CHECK: 1
  #     # NCCL_CUMEM_ENABLE: 0
  #     # NCCL_IB_DISABLE: 1
  #     HSA_ENABLE_IPC_MODE_LEGACY: 0
  #     # why not have all of them 

  #     HF_HUB_CACHE: /hf_home

  #   volumes:
  #     - /shared/huggingface:/hf_home
  #   command: ["/bin/sh", 
  #             "-c", 
  #             "vllm serve HURIDOCS/pdf-document-layout-analysis --port 8001 --gpu-memory-utilization 0.95
  #             "]
  #     # --tensor-parallel-size 2
  #     # --max-model-len 74782
  #     # Salesforce/Llama-xLAM-2-70b-fc-r
  #     #         --enable-auto-tool-choice
  #     #         --tool-parser-plugin ./xlam_tool_call_parser.py
  #     #         --tool-call-parser xlam
  open-webui:
    image: ghcr.io/open-webui/open-webui:main

    ports:
      - "2000:8080"

    restart: always
    ipc: "host"
    
    volumes:
      - /home/amysuo12/AMD2025VisionAgent/open-webui:/app/backend/data
      - /home/amysuo12/AMD2025VisionAgent/open-webui/images:/app/backend/data/images
    devices:
      - /dev/kfd
      - /dev/dri    
    environment:
      ENV: "dev"
      OPENAI_API_BASE_URL: "http://vllm:8001/v1"
      # what open webui needs to connect to vllm automatically
      ENABLE_OLLAMA_API: false
  
  mcp:
    build:
      context: .
      dockerfile: Dockerfile
    ports: 
      - "8002:8002"

    volumes: 
      - /home/amysuo12/AMD2025VisionAgent/open-webui/images:/mcp/images
    environment: 
      HIP_VISIBLE_DEVICES: 4
    
      
    command: ["mcpo", "--port", "8002", "--", "python", "server.py"]



  # kokoro-fastapi-cpu:
  #   ports:
  #       - 8880:8880
  #   image: ghcr.io/remsky/kokoro-fastapi-cpu
  #   restart: always